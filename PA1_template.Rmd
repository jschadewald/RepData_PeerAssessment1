---
title: "Reproducible Research: Peer Assessment 1"
output: 
  html_document:
    keep_md: true
---


## Quick Hello
Hi Reviewer!

Thanks for taking the time to consider my work. In case you wandered here by mistake, this is an assignment for the class "Reproducible Research" (repdata-016), which is offered by Johns Hopkins University via http://www.coursera.org.

From this point forward, I'll assume you're in the right place. One of the things I really like about R markdown is that it makes it very easy to show how the data is being transformed by the code in a step-by-step fashion. So, I'm taking this opportunity to go a little beyond the assignment by *showing* you what the code is doing rather than *telling* you what I think it's doing. The benefit is that there should be no question about *what* is actually happening or *whether* the code works. The risk to me is that the "big picture" might get lost in the details.

Ultimately, my goal is to make it as easy as possible for you to grade my assignment. So, I'll try to make the grade-able parts stand out for those of you who just want to rush through the checklist. Anything corresponding to one of the assessment questions is in **bold** (non-header) format and uses the language from the evaluation form as closely as possible. Aside from that, I'll do my best to make this entertaining.

One final important note before we proceed. Please take a moment to ensure you are looking at the `PA1_template.md` markdown file (and NOT the `PA1_template.Rmd` R markdown file) for the evaluation. This is important because only the .md file will render everything correctly in the browser on GitHub.

Happy reviewing!


## Loading and preprocessing the data
Since we're starting with a zipped file, we're going to need to unzip it first.  Following that, we will read the file and do some exploratory analysis. The exploratory results will give us a sense of whether any preprocessing is necessary. Last, we'll actually perform any needed preprocessing.

I'll assume, for convenience, that the activity.zip is in the same directory as this PA1_template.Rmd.

Note that the instructions for the evaluator at this point are to "Examine PA1_template.md" and find the code below. I hope you'll agree that "**There is code presented for reading in the data set.**"

```{r unzipAndRead, cache=TRUE}
#Read in the data set
unzip("activity.zip") #Note overwrite=TRUE is the default.
data<-read.csv("activity.csv", colClasses=c("integer","Date","integer"))
```

Now, let's see what it looks like before we preprocess it.
```{r lookaround}
summary(data)
head(data)
str(data)
```

At this point, "steps" and "date" make sense to me, but I want to take a closer look at "interval."

```{r intervalCheck}
head(data$interval, 30)
```

Okay.  See the jump from 55 to 100 and from 155 to 200?  Interval isn't counting minutes.  It's the time on that day. Furthermore, since it begins at 0, we can be fairly confident that the identifer represents the beginning of the interval.  In other words, "0" means "12:00:00am - 12:04:59am."

This is good to keep in mind, but at least for now, I'm satisfied that no preprocessing of the data is necessary.

## What is mean total number of steps taken per day?
The instructions for this part are:

1. Make a histogram of the total number of steps taken each day.
2. Calculate and report the **mean** and **median** total number of steps taken per day.

### Massaging the data for the histogram
In order to accomplish #1, we need to take the sum all of the steps for each day.  Let's make this easy by using reshape2. First, we'll create a molten data frame where "steps" is the only measure variable, and then we'll use dcast to sum over the dates.  Note that we can set na.rm=TRUE because the assignment expressly says "For this part of the assignment, you can ignore the missing values in the dataset." We'll store the resulting data frame in a new variable, hist_data.
```{r reshapeForHistogram1}
library(reshape2)
hist_data<-melt(data, id=c("date", "interval"), na.rm=TRUE)
hist_data<-dcast(hist_data, date ~ variable, sum)
head(hist_data)
```

Immediately, we can see that there was no data for the entirety of 2012-10-01.  Let's also take a moment to manually confirm that 12116 is the correct number of steps taken on 2012-10-04 as follows.
```{r confirm20121004}
data[864:865,] #beginning of 2010-10-04 is row 865
data[1152:1153,] #end of 2010-10-04 is row 1152
sum(data[865:1152,1])==12116
```

Given our knowledge of R and the evidence above, we should now be in agreement that hist_data contains the correct values.

### The histogram we've all been waiting for
*"Make a histogram of the total number of steps taken each day."*

I'll be using the ggplot2 library for all of my plots in this assignment. Without further ado, **here's the histogram**.
```{r histogram1}
library(ggplot2)
hist<-qplot(hist_data$steps,
            xlab="Number of Steps Taken in One Day",
            ylab="Number of Days with x Steps",
            main="Distribution of Steps in a Day")
print(hist)
```

Notice that I also saved the plot object into the variable "hist" so that we can use it again later.

### Mean and Median Steps over Days
*"Calculate and report the mean and median total number of steps taken per day."*

I will re-word that requirement to better indicate my interpretation:
"Give the single mean and median taken over all days using the same data from the histogram." The key phrase in the requirement that leads to my interpretation is "total number of steps." Below, you will see that **"both the mean and median are reported"**.
```{r meanAndMedian}
mean(hist_data$steps)
median(hist_data$steps)
```


For fun, let's throw those lines onto the histogram.
```{r histogram_for_fun}
hist + geom_vline(aes(xintercept=mean(hist_data$steps)),
                  color="red", linetype="solid", size=1) +
       geom_vline(aes(xintercept=median(hist_data$steps)),
                  color="blue", linetype="dashed", size=1)
```

I decided not to add a legend because it should be clear from context what the lines mean and because ggplot() is much more suited to manual tweaks than qplot() is. In fact, Dr. Roger Peng indicated in the video lectures that it's best to use ggplot() instead of qplot() if you plan to make any manual adjustments.


## What is the average daily activity pattern?
The point of this part of the assignment is to determine at what time this person does the most walking/running on "an average day." For that, we need to plot a slightly different data set than for the previous section. Let's call this one "avgday_data."

Again, I'm ignoring NA values, as permitted.
```{r makeDataForTimeSeries}
#library(reshape2) #Already loaded above.
avgday_data<-melt(data, id=c("date", "interval"), na.rm=TRUE)
avgday_data<-dcast(avgday_data, interval ~ variable, mean)
summary(avgday_data)
head(avgday_data)
tail(avgday_data)
```

Alright, avgday_data looks like what we'd expect if the transformation was done correctly. Let's get to the plotting.  Here is **a time series plot of the average number of steps taken (averaged across all days) versus the 5-minute intervals**.  For fun, it's lime green, and I set size=1 so that it is easier to see.
```{r timeSeries}
g<-ggplot(avgday_data, aes(interval,steps))
g + geom_line(color="limegreen", size=1) +
    labs(x="Time of Day", y="Average Steps",
         title="Steps per 5-minute Interval on an Average Day")
```

Judging from the graph, I'd guess this person goes for a morning jog on a fairly regular basis.

So, which 5-minute interval contains the maximum average number of steps? Well, we know that the maximum is 206.170 from our summary above the graph.  We also know from the graph that we expect the peak to fall between 8am and 9am. Here's the code to find it.
```{r maxAvgInterval}
#indicator is TRUE only for the row we want
indicator<-avgday_data$steps==max(avgday_data$steps)
avgday_data$interval[indicator] #The correct answer
```

As seen above, **the 5-minute interval containing the maximum average number of steps** is the 8:35am-8:40am interval.

## Imputing missing values
"Imputing missing values" has four sub-sections:

1. Report number of NAs in original data set
2. Explain how I plan to fill in the missing NA values with numbers
3. Actually go ahead and fill in the missing NA values with numbers
4. Repeat everything from the histogram section with the filled-in data

I already reported the NAs way at the beginning with summary(data), but let's do it a different way this time.
```{r countNAs}
numNA<-sum(is.na(data$steps))
numNA
```
There we go.


### How I'll fill in the NAs with numbers
The assignment indicates "the strategy need not be sophisticated." Before we devise a strategy, though, let's take a closer look at how the NAs are distributed within the data set.

Which dates have NA values?
```{r datesWithNAs}
missingdates<-unique(data[is.na(data$steps),2])
missingdates
```

Okay, only 8 dates and a total of 2304 NA values. Wait a minute. We don't have scattered NA values; we have entire days without data!  Check it out:
```{r missingData}
#Number of 5-minute intervals in a day
fiveMinInts<-24*60/5

#Number of NAs per date with missing data
NAsPerMissingDay<-numNA/length(missingdates)

#These two are equal only if the entire day is missing for all 8 days
fiveMinInts
NAsPerMissingDay
```
There you have it. That's going to limit the strategies that we can apply.


With the above information, I'll use the example strategy of filling in the NAs by using the mean for the interval over all days. In other words, my **description of a strategy for imputing missing data** is that I will replace each NA value with the corresponding average for that interval from avgday_data, rounded to the nearest integer (i.e. no decimals).  Since this strategy was taken directly from a suggestion in the instructions, it should be allowable as long as it is possible.

### Create the new data set with filled-in values
Here's the code that does the work of defining a new "imputed_data" dataset according to the strategy above. Following these two lines of code is an explanation of why it works.
```{r newdataset}
imputed_data<-data #First make a copy
imputed_data$steps[is.na(imputed_data$steps)]<-round(avgday_data$steps)
```

The cool thing about the above code is that it works exactly because the NAs are distributed into entire-day blocks and because R does that nice "vector recycling" thing. As shown previously, each day has 288 5-minute intervals, and each missing day therefore has 288 NA values. Importantly, the intervals are in the same order (0, 5, 10, etc.) in both `imputed_data` and `avgday_data`, so the assignment of values acts the way that we want it to instead of scrambling everything around.  But, enough talk.  How about some proof?

Proof that We filled in all of the NA values:
```{r proof1}
sum(is.na(imputed_data$steps))
```
It's zero, so there are no NAs left.

Proof that the non-NA data is exactly the same:
```{r proof2}
NAorig<-is.na(data$steps)
sum(!imputed_data[!NAorig,]==data[!NAorig,])
```
Again, it's zero, so all non-NA rows from the original data set are exactly equal to their corresponding rows in the imputed data set.

Lastly, proof that the data mapped correctly onto the NA values:
```{r proof3}
#Basically, this is an equivalent way to assign the values.
newvalues<-rep(round(avgday_data$steps), length(missingdates))
sum(!imputed_data$steps[NAorig]==newvalues)
```
Zero again. All's good in the neighborhood. :)

### Histogram, Mean, and Median with Imputed Data
As mentioned earlier, this part is just a repeat of the first histogram, mean, and median with the added twist that it uses the imputed data set.  So, I copy pasted the R markdown sections `reshapeForHistogram1` and `histogram1` and then replaced these variable names with new ones: hist_data, data, hist. I also removed the na.rm=TRUE option because we no longer need it.

```{r reshapeForHistogram2}
#library(reshape2) #Already loaded
hist_data_i<-melt(imputed_data, id=c("date", "interval"))
hist_data_i<-dcast(hist_data_i, date ~ variable, sum)
head(hist_data_i)
```

...and here is the **histogram of the total number of steps taken each day after missing values were imputed**.
```{r histogram2}
#library(ggplot2) #Already loaded
hist_i<-qplot(hist_data_i$steps,
            xlab="Number of Steps Taken in One Day (Imputed)",
            ylab="Number of Days with x Steps (Imputed)",
            main="Imputed Distribution of Steps in a Day")
print(hist_i)
```


Also, the instructions call for calcuating and reporting the mean and median again as well as some "thinking out loud" about the impact of tweaking the data, but there is no corresponding grading criteria. When I'm grading someone else, I won't require this part, but here it is anyway.
```{r meanAndMedianAgain}
mean(hist_data_i$steps)
median(hist_data_i$steps)
```

Not surprisingly, these estimates do differ from the first part of the assignment. The new mean is closer to the previous median, and the new median has dropped. This is because the (rounded!) number of steps in our interval-averaged day is less than the average number of steps in a day (as computed with the ORIGINAL non-imputed data).  I'll rephrase that last sentence in code for clarity.
```{r rephrase}
#NOTE: I'm intentionally comparing the non-imputed data sets.
sum(avgday_data$steps) == mean(hist_data$steps) #Equal without rounding
sum(round(avgday_data$steps)) < mean(hist_data$steps) #Rounding has an effect
sum(round(avgday_data$steps)) - sum(avgday_data$steps) #How much of an effect?
```
So, my decision to round the numbers had a measurable impact. Pretty good lesson there about the dangers of rounding. Now, I'm even happier about the decision to round because it gave this assignment some additional value.

The other lesson here is more general: Be very careful about imputing data. It's easy to get it wrong and ruin your analysis. If you are going to impute data, then do a really thorough analysis of the non-imputed data, make sure to contextualize your use of the imputed data (i.e. certain strategies will be okay for some specialized needs but not for others), and document everything for the world to see.

## Are there differences in activity patterns between weekdays and weekends?
Almost done! Now is the part where we add a new column to the imputed data set that indicates whether it's a weekday or weekend, and then we use it to compare weekday versus weekend activity using a top-bottom time-series panel plot.

Here's the code to add the new column to the imputed data.
```{r addWeekendIndicator1}
#Logical indicator. TRUE for weekend days.
weekend<-weekdays(imputed_data$date) %in% c("Saturday","Sunday")
weekend<-as.factor(weekend)
levels(weekend)
```
Notice that the levels are FALSE, TRUE and that we want to assign "weekday" to FALSE and "weekend" to TRUE.  Let's do that explicitly and then add the new weekend factor directly to the imputed data set.

```{r addWeekendIndicator2}
levels(weekend)<-c("weekday","weekend")
imputed_data$weekend<-weekend
head(imputed_data)

#For good measure, let's also pick a weekend and show that it's labeled right.
imputed_data[1440:1441,]
```


With the observations appropriately labeled, the next step is to compute the "average weekday" and "average weekend" values for each interval (like we did much earlier for the "average day" time series).
```{r makeDataForTimeSeries2}
#library(reshape2) #Already loaded above.
avgday_data2<-melt(imputed_data, id=c("date", "interval", "weekend"))
avgday_data2<-dcast(avgday_data2, interval + weekend ~ variable, mean)
summary(avgday_data2)
head(avgday_data2)
tail(avgday_data2)
```


Looking good! Let's graph it. Here is **a panel plot comparing the average number of steps taken per 5-minute interval across weekdays and weekends**.
```{r panelplot}
#library(ggplot2) #Already loaded above
g2<-ggplot(avgday_data2, aes(interval, steps))
g2 + geom_line(color="limegreen", size=1) +
    facet_grid(weekend ~ .) +
    labs(x="Time of Day", y="Average Steps",
         title="Steps per 5-minute Interval on an Average Day")
```

So, it appears that this individual does most of their walking/running on weekdays. Note here that the dissimilarities between the above plot and the example plot are due to two factors: use of ggplot2 rather than lattice, and use of the actual data set rather than randomized simulated data.

## Conclusion
Thanks again for taking the time to review. I'm confident that I have done everything required to receive full credit, and I hope you will agree. Beyond that, I will absolutely appreciate any constructive, kind feedback you can offer about ways to improve organization, further clarify intent, or otherwise create a better experience for future reviewers.

Thanks!